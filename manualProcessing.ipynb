{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import  RandomUnderSampler, RepeatedEditedNearestNeighbours\n",
    "from sklearn.datasets import make_classification\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "DATA_PATH = Path.cwd().parent / \"ML project\" \n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import eli5\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import xgboost as xgb\n",
    "from itertools import combinations\n",
    "RANDOM_SEED = 6    # Set a random seed for reproducibility!\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "def distanceLong(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    latDiff = (lat2-lat1)*p\n",
    "    lonDiff = (lon2-lon1)*p\n",
    "    coslat1 = np.cos(lat1*p)\n",
    "    coslat2 = np.cos(lat2*p)\n",
    "    coslondiff = np.cos(lonDiff)\n",
    "    coslatdiff = np.cos(latDiff)\n",
    "    a = 0.5 - coslatdiff/2 + coslat1 * coslat2 * (1-coslondiff)/2\n",
    "    return 12742 * (np.arcsin(a**0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def formatdata(dataFrame):\n",
    "    dataFrame['pickup_time'] = dataFrame['pickup_time']+ '.099'\n",
    "    dataFrame['pickup_time'] = dataFrame['pickup_time'].apply(lambda x:  x.replace(\"/\",\"-\"))\n",
    "    dataFrame['drop_time'] = dataFrame['drop_time']+ '.099'\n",
    "\n",
    "    pd.to_datetime(dataFrame['pickup_time'], format='%m-%d-%Y %H:%M:%S.%f', errors='ignore')\n",
    "    pd.to_datetime(dataFrame['drop_time'], format='%m/%d/%Y %H:%M:%S.%f', errors='ignore')\n",
    "    \n",
    "    dataFrame['pickup_time'] = dataFrame['pickup_time'].apply(lambda x: datetime.timestamp(datetime.strptime(x,'%m-%d-%Y %H:%M.%f')))\n",
    "    dataFrame['drop_time'] = dataFrame['drop_time'].apply(lambda x: datetime.timestamp(datetime.strptime(x,'%m/%d/%Y %H:%M.%f')))\n",
    "\n",
    "    dataFrame['timediff'] =  dataFrame['drop_time'] - dataFrame['pickup_time']\n",
    "    #dataFrame['timedifferant'] =  dataFrame['drop_time'] - dataFrame['pickup_time'] + dataFrame['duration']\n",
    "    dataFrame['distance'] = distanceLong(dataFrame['drop_lat'],dataFrame['drop_lon'],dataFrame['pick_lat'],dataFrame['pick_lon'])*100\n",
    "    #dataFrame['distance'] =  ((dataFrame['drop_lat']-dataFrame['pick_lat'])**2+(dataFrame['drop_lon']-dataFrame['pick_lon'])**2)**0.5\n",
    "    #dataFrame['unit_meter_waiting_fair'] = dataFrame['meter_waiting_fare'] / dataFrame['meter_waiting']\n",
    "    #print(dataFrame['unit_meter_waiting_fair'])\n",
    "    \n",
    "    #dataFrame['_fare'] = dataFrame['fare'] - dataFrame['additional_fare'] \n",
    "    #dataFrame['tripDuration'] = dataFrame['duration'] - dataFrame['meter_waiting']\n",
    "    \n",
    "    \n",
    "    return dataFrame\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatFeatures(dataFrame, datatype = 0):\n",
    "    df = dataFrame\n",
    "    \n",
    "    all_features = df.columns.values\n",
    "    \n",
    "    df.replace({999.0: np.NaN}, inplace=True)\n",
    "    indicator = MissingIndicator(missing_values=np.NaN)\n",
    "    indicator = indicator.fit_transform(df)\n",
    "    \n",
    "    indicator = pd.DataFrame(indicator,columns=[\"ind\"+str(i) for i in range(indicator.shape[1])], index=df.index)\n",
    "    \n",
    "    imp_numerical = SimpleImputer(strategy='mean',missing_values=np.nan)\n",
    "    imp_catagorical = SimpleImputer(strategy='constant',missing_values=np.nan)\n",
    "    \n",
    "    df[df.columns[df.dtypes != \"object\"].values] = imp_numerical.fit_transform(df[df.columns[df.dtypes != \"object\"].values])\n",
    "    #df[df.columns[df.dtypes == \"object\"].values] = imp_catagorical.fit_transform(df[df.columns[df.dtypes == \"object\"].values])\n",
    "    \n",
    "    numerical_features = df.columns[df.dtypes != \"object\"]\n",
    "    allColsToPoly = df.columns.values\n",
    "    polyList = [['additional_fare','meter_waiting_fare'],['additional_fare', 'fare'],\n",
    "                ['distance','timediff'],['timediff','meter_waiting'],['timediff','duration'],['fare','timediff']]\n",
    "    poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "    #polynomials = pd.DataFrame(poly\n",
    "     #                          .fit_transform(df[]),\n",
    "      #                         columns=[\"poly\"+str(i) for i in range(4)],index=df.index)\n",
    "    \n",
    "    #polynomials = polynomials.drop(['poly0'],axis=1)\n",
    "    df = pd.concat([df, indicator], axis=1)\n",
    "    for i in polyList:\n",
    "                \n",
    "        colName = ''\n",
    "        for j in i:\n",
    "            colName +=j+\"_\"\n",
    "        poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "        polynomials = pd.DataFrame(poly\n",
    "                                    .fit_transform(df[list(i)]),\n",
    "                                    columns=[colName+str(i) for i in range(4)],index=df.index)\n",
    "\n",
    "        polynomials = polynomials.drop([colName+'0'],axis=1) \n",
    "        df = pd.concat([df, polynomials], axis=1)\n",
    "   \n",
    "    #continuesList = ['duration','meter_waiting','meter_waiting_fare','meter_waiting_till_pickup','fare']\n",
    "    #continuesList=[]\n",
    "    #for i in continuesList:\n",
    "     #   disc = KBinsDiscretizer(n_bins=3, encode='ordinal', \n",
    "      #                          strategy='uniform')\n",
    "        #df[i] = disc.fit_transform(df[i].values.reshape(-1,1))\n",
    "    \n",
    "    allCols = df.columns.values\n",
    "    for i in allCols:\n",
    "        scaler = StandardScaler()\n",
    "        df[i] = scaler.fit_transform(df[i].values.reshape(-1,1))\n",
    "    features_df_norm_l2 = list(math.sqrt(sum(list((i**2) for i in df.iloc[r]))) \n",
    "                           for r in range(len(df)))\n",
    "    \n",
    "    indexCount = 0\n",
    "    for index, row in df.iterrows():\n",
    "        rowt = row/features_df_norm_l2[indexCount]\n",
    "        df.loc[index] = rowt\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ac12adb103fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_df = pd.read_csv(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mDATA_PATH\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tripid\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
    "    DATA_PATH / \"train.csv\", \n",
    "    index_col=\"tripid\"\n",
    ")\n",
    "train_df = train_df.drop_duplicates()\n",
    "\n",
    "\n",
    "train_df['label'] = train_df['label'].map({\"correct\":1, \"incorrect\":0})\n",
    "train_df_cols = train_df.columns.values\n",
    "li = list(train_df_cols)\n",
    "features_df = train_df[li[0:-1]]\n",
    "labels_df = train_df[li[-1]]\n",
    "\n",
    "\n",
    "\n",
    "features_df = formatdata(features_df)\n",
    "\n",
    "features_df = features_df.drop(['drop_lon','pick_lon','drop_lat','pick_lat','pickup_time','drop_time'],axis=1)\n",
    "features_df = formatFeatures(features_df)\n",
    "\n",
    "\n",
    "numeric_cols = features_df.columns[features_df.dtypes != object].values\n",
    "\n",
    "numeric_preprocessing_steps = Pipeline([\n",
    "    ('standard_scaler', StandardScaler()),\n",
    "    ('simple_imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "#categorical_transformer = Pipeline(steps=[\n",
    " #   ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "  #  ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# create the preprocessor stage of final pipeline\n",
    "# each entry in the transformer list is a tuple of\n",
    "# (name you choose, sklearn transformer, list of columns)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        (\"numeric\", numeric_preprocessing_steps, numeric_cols)       \n",
    "    ],\n",
    "    remainder = \"drop\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    5275\n",
      "0     392\n",
      "Name: prediction, dtype: int64\n",
      "0.9744922514197709\n"
     ]
    }
   ],
   "source": [
    "solver = 'lbfgs'\n",
    "alpha = 2\n",
    "hidden_layer_sizes=(7,2 )\n",
    "random_state=2\n",
    "max_iter= 100000000\n",
    "\n",
    "activation='tanh'\n",
    "#estimators=MLPClassifier(solver=solver, alpha=alpha, hidden_layer_sizes=hidden_layer_sizes, random_state=random_state,\n",
    " #                        max_iter=max_iter, activation=activation)\n",
    "min_child_weight=2\n",
    "max_depth=5\n",
    "gamma=0.02\n",
    "colsample_bytree=0.8\n",
    "subsample=0.8\n",
    "n_estimators=200\n",
    "threads = 8\n",
    "estimators = XGBClassifier(min_child_weight=min_child_weight,max_depth=max_depth, \n",
    "                           gamma=gamma,colsample_bytree=colsample_bytree,subsample=subsample,\n",
    "                           n_estimators=n_estimators, n_jobs=threads\n",
    "                          )\n",
    "#estimators = BaggingClassifier(base_estimator=estimatorsY,n_estimators=10,max_samples=1.0,\n",
    " #                            max_features=1.0, random_state=1,bootstrap=False)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    features_df,\n",
    "    labels_df,\n",
    "    test_size=0.33,\n",
    "    shuffle=True,\n",
    "    stratify=labels_df,\n",
    "    random_state=2\n",
    ")\n",
    "full_pipelinea = Pipeline([\n",
    "    \n",
    "    (\"estimators\", estimators),\n",
    "])\n",
    "\n",
    "estimators.fit(X_train, y_train)\n",
    "preds = estimators.predict(X_eval)\n",
    "y_preds = pd.DataFrame(\n",
    "    {'prediction':preds},\n",
    "    index = y_eval.index\n",
    ")\n",
    "\n",
    "f1 = f1_score(y_eval, y_preds)\n",
    "print(y_preds.prediction.value_counts())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    5464\n",
      "0    3112\n",
      "Name: prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_features_df = pd.read_csv(DATA_PATH / \"test.csv\", \n",
    "                               index_col=\"tripid\")\n",
    "\n",
    "test_features_df = formatdata(test_features_df)\n",
    "test_features_df = test_features_df.drop(['drop_lon','pick_lon','drop_lat','pick_lat','pickup_time','drop_time'],axis=1)\n",
    "test_features_df = formatFeatures(test_features_df)\n",
    "test_features_df['ind2'] = False\n",
    "test_features_df['ind3'] = False\n",
    "test_features_df['ind4'] = False\n",
    "test_features_df['ind5'] = False\n",
    "test_probas = estimators.predict(test_features_df)\n",
    "test_preds = pd.DataFrame(\n",
    "    {'prediction':test_probas},\n",
    "    index = test_features_df.index\n",
    ")\n",
    "\n",
    "print(test_preds.prediction.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    8031\n",
      "0     545\n",
      "Name: prediction, dtype: int64\n",
      "0.9524279023909293\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.prediction.value_counts())\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.to_csv(DATA_PATH / 'my_submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, xtrain,ytrain, predictors, title,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "        xtrain,\n",
    "        ytrain,\n",
    "        test_size=0.33,\n",
    "        shuffle=True,\n",
    "        stratify=ytrain,\n",
    "        random_state=6\n",
    "    )\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(xtrain[predictors].values, label=ytrain.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    \n",
    "    alg.fit(X_train[predictors], y_train,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_pred = alg.predict(X_train[predictors])\n",
    "    deval_pred = alg.predict(X_eval[predictors])\n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report: \"+title)\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_train, dtrain_pred))\n",
    "    print (\"f1 Score (Train): %f\" % metrics.f1_score(y_train, dtrain_pred))\n",
    "    print (\"f1 Score (Test): %f\" % metrics.f1_score(y_eval, deval_pred))\n",
    "    feat_imp = pd.Series(alg.feature_importances_)\n",
    "    feat_imp.plot(kind='bar', title=title)\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = features_df.columns.values\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=100,\n",
    " max_depth=6,\n",
    " min_child_weight=3,\n",
    " gamma=0.1,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=8,\n",
    " n_jobs=8,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "#modelfit(xgb1, features_df, labels_df, predictors, 'fareinfare')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = list(features_df.columns.values)\n",
    "for i in range(len(colList)):\n",
    "    print (i, colList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#estimators = MultiOutputClassifier(\n",
    " #   estimator=MLPClassifier(solver='lbfgs', alpha=.55, hidden_layer_sizes=(4,2 ), random_state=1, max_iter=10000000)\n",
    "#)\n",
    "#estimators = MultiOutputClassifier(\n",
    " #   estimator=SVC(gamma='auto', probability=True),\n",
    "#)\n",
    "#estimators =SVC(gamma='auto')\n",
    "#estimatorsY=LogisticRegression(penalty=\"l2\", C=20,solver='lbfgs',max_iter=1000000)\n",
    "solver = 'lbfgs'\n",
    "alpha = 0.075\n",
    "hidden_layer_sizes=(7,2 )\n",
    "random_state=80\n",
    "max_iter= 10000000\n",
    "\n",
    "activation='tanh'\n",
    "estimators=MLPClassifier(solver=solver, alpha=alpha, hidden_layer_sizes=hidden_layer_sizes, random_state=random_state,\n",
    "                         max_iter=max_iter, activation=activation)\n",
    "\n",
    "\n",
    "#estimatorsX2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, max_features=2, max_depth=1, random_state=0)\n",
    "min_child_weight=2\n",
    "max_depth=5\n",
    "gamma=0.5\n",
    "colsample_bytree=0.75\n",
    "subsample=0.75\n",
    "n_estimators=5000\n",
    "threads = 8\n",
    "#estimators = XGBClassifier(min_child_weight=min_child_weight,max_depth=max_depth, \n",
    " #                          gamma=gamma,colsample_bytree=colsample_bytree,subsample=subsample,\n",
    "  #                         n_estimators=n_estimators, n_jobs=threads\n",
    "   #                       )\n",
    "#params = { 'axgb__n_estimators': [20, 200],'cmlp__alpha':[0,1.0],'blr__C': [1.0, 100.0]}\n",
    "#estimatorsX3 = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate = 0.05, \n",
    "#                                algorithm='SAMME.R',base_estimator=estimatorsX)\n",
    "\n",
    "#estimators = RandomForestClassifier(n_estimators=50, criterion='entropy',max_depth=10,min_samples_split=4,\n",
    " #                                  min_samples_leaf=4,max_features='log2',max_leaf_nodes=100,bootstrap=True,\n",
    "  #                                  oob_score=True,class_weight='balanced_subsample')\n",
    "#estimatorNB = GaussianNB()\n",
    "\n",
    "#estimatorsY = LGBMClassifier(boosting_type='gbdt', num_leaves=40, max_depth=6,learning_rate=0.2,\n",
    " #                          n_estimators=1000,random_state=42)\n",
    "\n",
    "#estimators = KNeighborsClassifier(n_neighbors=5, weights='distance',algorithm='brute',p=2 ,leaf_size=10)\n",
    "#estimators = BaggingClassifier(base_estimator=estimatorsY,n_estimators=10,max_samples=1.0,\n",
    " #                            max_features=1.0, random_state=1,bootstrap=False)\n",
    "#estimators = VotingClassifier( [('xgb', estimatorsX), ('mlp', estimatorsY)], weights=[1,1])\n",
    "#estimators = GridSearchCV(estimator=vestimators, param_grid=params, cv=5)\n",
    "#estimators = HistGradientBoostingClassifier(max_iter=500,min_samples_leaf=2,max_depth=10, learning_rate=0.08)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = 'lbfgs'\n",
    "alpha = 0.075\n",
    "hidden_layer_sizes=(7,2 )\n",
    "random_state=2\n",
    "max_iter= 10000000\n",
    "\n",
    "activation='tanh'\n",
    "estimators=MLPClassifier(solver=solver, alpha=alpha, hidden_layer_sizes=hidden_layer_sizes, random_state=random_state,\n",
    "                         max_iter=max_iter, activation=activation)\n",
    "min_child_weight=2\n",
    "max_depth=5\n",
    "gamma=0.01\n",
    "colsample_bytree=0.75\n",
    "subsample=0.75\n",
    "n_estimators=165\n",
    "threads = 8\n",
    "estimatorsY = XGBClassifier(min_child_weight=min_child_weight,max_depth=max_depth, \n",
    "                           gamma=gamma,colsample_bytree=colsample_bytree,subsample=subsample,\n",
    "                           n_estimators=n_estimators, n_jobs=threads\n",
    "                          )\n",
    "#estimators = BaggingClassifier(base_estimator=estimatorsY,n_estimators=10,max_samples=1.0,\n",
    " #                            max_features=1.0, random_state=1,bootstrap=False)\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    features_df,\n",
    "    labels_df,\n",
    "    test_size=0.20,\n",
    "    shuffle=True,\n",
    "    stratify=labels_df,\n",
    "    random_state=6\n",
    ")\n",
    "full_pipelinea = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", estimators),\n",
    "])\n",
    "\n",
    "estimators.fit(X_train, y_train)\n",
    "preds = estimators.predict(X_eval)\n",
    "y_preds = pd.DataFrame(\n",
    "    {'prediction':preds},\n",
    "    index = y_eval.index\n",
    ")\n",
    "\n",
    "f1 = f1_score(y_eval, y_preds)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df = pd.read_csv(DATA_PATH / \"test.csv\", \n",
    "                               index_col=\"tripid\")\n",
    "\n",
    "test_features_df = formatdata(test_features_df)\n",
    "test_features_df = test_features_df.drop(['drop_lon','pick_lon','drop_lat','pick_lat','pickup_time','drop_time'],axis=1)\n",
    "test_features_df = formatFeatures(test_features_df)\n",
    "test_probas = estimators.predict(test_features_df)\n",
    "test_preds = pd.DataFrame(\n",
    "    {'prediction':test_probas},\n",
    "    index = test_features_df.index\n",
    ")\n",
    "print(test_preds.prediction.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    8345\n",
      "0     231\n",
      "Name: prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.prediction.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975378787878788\n",
      "0.9759036144578315\n",
      "0.9735466497703152\n",
      "0.9700541573749603\n",
      "0.9721647844759027\n",
      "bestscore 0.9759036144578315\n",
      "random state 80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "bestModel = ''\n",
    "bestscore = 0\n",
    "scores = []\n",
    "random_s = 0\n",
    "for i in range(80,81):\n",
    "    \n",
    "    kf = KFold(n_splits=5, random_state=i, shuffle=True)\n",
    "\n",
    "    kf.get_n_splits(features_df)\n",
    "\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(features_df):\n",
    "        X_train, X_eval, y_train, y_eval = features_df.iloc[train_index], features_df.iloc[test_index],labels_df.iloc[train_index], labels_df.iloc[test_index]\n",
    "        \n",
    "        estimatorsY = XGBClassifier(min_child_weight=min_child_weight,max_depth=max_depth, \n",
    "                                   gamma=gamma,colsample_bytree=colsample_bytree,subsample=subsample,\n",
    "                                   n_estimators=n_estimators, n_jobs=threads\n",
    "                                  )\n",
    "        estimators = BaggingClassifier(base_estimator=estimatorsY,n_estimators=10,max_samples=1.0,\n",
    "                                     max_features=1.0, random_state=1,bootstrap=False)\n",
    "        estimators.fit(X_train, y_train)\n",
    "        preds = estimators.predict(X_eval)\n",
    "        y_preds = pd.DataFrame(\n",
    "            preds,\n",
    "            index = y_eval.index\n",
    "        )\n",
    "\n",
    "        f1 = f1_score(y_eval, y_preds)\n",
    "        scores.append(f1)\n",
    "        if(f1 > bestscore):\n",
    "            bestscore = f1\n",
    "            bestModel = estimators\n",
    "            random_s = i\n",
    "        print(f1)\n",
    "\n",
    "print('bestscore', bestscore)\n",
    "print('random state', random_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bestscore',bestscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df = pd.read_csv(DATA_PATH / \"test.csv\", \n",
    "                               index_col=\"tripid\")\n",
    "\n",
    "test_features_df = formatdata(test_features_df)\n",
    "test_features_df = test_features_df.drop(['drop_lon','pick_lon','drop_lat','pick_lat','pickup_time','drop_time'],axis=1)\n",
    "test_features_df = formatFeatures(test_features_df)\n",
    "test_probas = bestModel.predict(test_features_df)\n",
    "test_preds = pd.DataFrame(\n",
    "    {'prediction':test_probas},\n",
    "    index = test_features_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probas = bestModel.predict(test_features_df)\n",
    "\n",
    "test_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test_preds = pd.DataFrame(\n",
    "    {'prediction':test_probas},\n",
    "    index = test_features_df.index\n",
    ")\n",
    "\n",
    "print(test_preds['prediction'].max())\n",
    "print(test_preds['prediction'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.to_csv(DATA_PATH / 'my_submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the rows in the same order\n",
    "np.testing.assert_array_equal(test_features_df.index.values, \n",
    "                              submission_df.index.values)\n",
    "\n",
    "# Save predictions to submission data frame\n",
    "submission_df[\"h1n1_vaccine\"] = test_preds[1]\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(DATA_PATH / 'my_submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "to = '11/01/2019  00:20:00'\n",
    "to += '.099'\n",
    "froms = '11/01/2019  00:34:00'+'.099'\n",
    "dateObjectto = datetime.strptime(to, '%m/%d/%Y %H:%M:%S.%f')\n",
    "dateObjectfrom = datetime.strptime(froms, '%m/%d/%Y %H:%M:%S.%f')\n",
    "print(datetime.timestamp(dateObjectfrom) - datetime.timestamp(dateObjectto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    DATA_PATH / \"train.csv\", \n",
    "    index_col=\"tripid\"\n",
    ")\n",
    "train_df['label'] = train_df['label'].map({\"correct\":1, \"incorrect\":0})\n",
    "train_df_cols = train_df.columns.values\n",
    "li = list(train_df_cols)\n",
    "features_df = train_df[li[0:-1]]\n",
    "labels_df = train_df[li[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features_df[features_df.dtypes[(features_df.dtypes==\"float64\")]\n",
    "                        .index.values].hist(figsize=[11,11],\n",
    "                                            range=[features_df['drop_lat'].min(), features_df['drop_lat'].max()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "cols_to_norm = ['duration','meter_waiting',\n",
    "                'meter_waiting_fare','meter_waiting_till_pickup',\n",
    "                'pick_lat','pick_lon','drop_lat','drop_lon','fare']\n",
    "n_test = features_df[cols_to_norm]\n",
    "\n",
    "\n",
    "x = n_test.values\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "\n",
    "n_test = pd.DataFrame(x_scaled, columns=cols_to_norm,index=features_df.index)\n",
    "\n",
    "l_test = features_df.drop(cols_to_norm, axis=1)\n",
    "train = pd.concat([n_test, l_test], axis=1)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(features_df['meter_waiting_fare'], labels_df,\n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    \n",
    "    plt.xlabel('Alcohol')\n",
    "    plt.ylabel('Malic Acid')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "train_df = pd.read_csv(\n",
    "    DATA_PATH / \"train.csv\", \n",
    "    index_col=\"tripid\"\n",
    ")\n",
    "train_df['label'] = train_df['label'].map({\"correct\":1, \"incorrect\":0})\n",
    "train_df_cols = train_df.columns.values\n",
    "li = list(train_df_cols)\n",
    "features_df = train_df[li[0:-1]]\n",
    "labels_df = train_df[li[-1]]\n",
    "features_df = formatdata(features_df)\n",
    "features_df = features_df.join(labels_df)\n",
    "\n",
    "labels_df.fillna(0)\n",
    "features_df[features_df==np.inf]=np.nan\n",
    "features_df.fillna(0, inplace=True)\n",
    "numericOnle = ['duration','meter_waiting','meter_waiting_fare','meter_waiting_till_pickup','fare','timediff','distance','label']\n",
    "features_set = features_df[numericOnle]\n",
    "\n",
    "#get correlations of each features in dataset\n",
    "corrmat = features_set.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(8,8))\n",
    "#plot heat map\n",
    "g=sns.heatmap(features_set[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(__doc__)\n",
    "\n",
    "# Author: Mathieu Blondel\n",
    "#         Jake Vanderplas\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\" function to approximate by polynomial interpolation\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "X, Y = features_df, labels_df\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    features_df,\n",
    "    labels_df,\n",
    "    test_size=0.33,\n",
    "    shuffle=True,\n",
    "    stratify=labels_df,\n",
    "    random_state=6\n",
    ")\n",
    "colors = ['teal', 'yellowgreen', 'gold']\n",
    "lw = 2\n",
    "plt.plot(X, f(X), color='cornflowerblue', linewidth=lw,\n",
    "         label=\"ground truth\")\n",
    "plt.scatter(X_train['timediff'], y_train, color='navy', s=20, marker='o', label=\"training points\")\n",
    "\n",
    "for count, degree in enumerate([3, 4, 5]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_plot = model.predict(X_eval)\n",
    "    plt.plot(X_train, y_train, color=colors[count], linewidth=lw,\n",
    "             label=\"degree %d\" % degree)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
